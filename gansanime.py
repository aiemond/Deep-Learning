# -*- coding: utf-8 -*-
"""GANsAnime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14kPFk0zNt9qIsAEiCwIiUe4TAVmQymRl
"""

!pip install opendatasets --upgrade --quiet

import opendatasets as od

dataset_url = 'https://www.kaggle.com/datasets/splcher/animefacedataset'
od.download(dataset_url)

import os
DATA_DIR = './animefacedataset'
print(os.listdir(DATA_DIR))

print(os.listdir('/content/animefacedataset/images')[:5])
#print(os.listdir(DATA_DIR+'/images')[:5])

from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torchvision.transforms as T

image_size = 64
batch_size = 128
stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)

train_ds = ImageFolder(DATA_DIR, transform=T.Compose([
    T.Resize(image_size),
    T.CenterCrop(image_size),
    T.ToTensor(),
    T.Normalize(*stats)]))

train_dl = DataLoader(train_ds, 
                      batch_size, 
                      shuffle=True, 
                      num_workers=3, 
                      pin_memory=True)

# Commented out IPython magic to ensure Python compatibility.
import torch
from torchvision.utils import make_grid
import matplotlib.pyplot as plt
# %matplotlib inline

def denorm(img_tensors):
    return img_tensors * stats[1][0] + stats[0][0]

def show_images(images, nmax=64):
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.set_xticks([]); ax.set_yticks([])
    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))

def show_batch(dl, nmax=64):
    for images, _ in dl:
        show_images(images, nmax)
        break

show_batch(train_dl)

"""GPU or CPU"""

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')
    
def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl: 
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
device

train_dl = DeviceDataLoader (train_dl,device)

"""Defining discriminator Network"""

import torch.nn as nn
discriminator = nn.Sequential (
    #input 3*64*64
               nn.Conv2d (3,64, stride=2, kernel_size=4, padding=1,bias= False),
               nn.BatchNorm2d (64),
               nn.LeakyReLU (0.2), # activation function
               # output 64*32*32  

               nn.Conv2d (64,128, stride=2, kernel_size=4, padding=1,bias= False),
               nn.BatchNorm2d (128),
               nn.LeakyReLU (0.2), # activation function
               # output 128*16*16

               nn.Conv2d (128,256, stride=2, kernel_size=4, padding=1,bias= False),
               nn.BatchNorm2d (256),
               nn.LeakyReLU (0.2), # activation function

               nn.Conv2d (256,512, stride=2, kernel_size=4, padding=1,bias= False),
               nn.BatchNorm2d (512),
               nn.LeakyReLU (0.2), # activation function  

               nn.Conv2d (512,1, stride=1, kernel_size=4, padding=0,bias= False),
               # output 1*1*1
               nn.Flatten (), # convert 3D to 1D
               nn.Sigmoid () # convert any positive value to range (-1 to +1), for probability calculation
               )

discriminator = to_device (discriminator,device)

"""Defining Generator Network"""

from torch.nn.modules.activation import ReLU
latent_size =128

generator = nn.Sequential (
    #input =128*1*1
            nn.ConvTranspose2d (latent_size,512, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d (512),
            nn.ReLU (True),
            #out= 512*4*4
            nn.ConvTranspose2d (512,256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d (256),
            nn.ReLU (True),
            # out 256*8*8

            nn.ConvTranspose2d (256,128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d (128),
            nn.ReLU (True),
            # out 128*16*16

            nn.ConvTranspose2d (128,64, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d (64),
            nn.ReLU (True),
            # out 64*32*32

            nn.ConvTranspose2d (64,3, kernel_size=4, stride=2, padding=1, bias=False),
            # out 3*64*64
            nn.Tanh () #outputs of the TanH activation function lie in the range [-1,1]
)

random_pick = torch.rand (batch_size,latent_size,1,1) #random latent tensor
fake_images = generator (random_pick)
print (fake_images.shape)
show_images (fake_images)

generator = to_device (generator,device)

"""Discriminator Training"""

import torch.nn.functional as F
def train_discriminator (real_images, opt_d):
  opt_d.zero_grad () #clear discriminator gradient zero

  #pass real images through discriminator
  real_prediction = discriminator (real_images)
  real_targets = torch.ones (real_images.size (0), 1, device = device)
  real_loss = F.binary_cross_entropy (real_prediction,real_targets)
  real_score = torch.mean(real_prediction).item ()

  # generate fake images

  latent = torch.rand (batch_size,latent_size,1,1, device= device)
  fake_images = generator (latent)

  # pass fake images through discriminator
  fake_predictions = discriminator (fake_images)
  fake_targets = torch.zeros (fake_images.size(0),1,device = device)
  fake_loss = F.binary_cross_entropy (fake_predictions,fake_targets)
  fake_score = torch.mean (fake_predictions).item()

  #update discriminator weights
  loss = real_loss + fake_loss
  loss.backward()
  opt_d.step()
  return loss.item() ,real_score, fake_score

"""Generator Training"""

def train_generator (opt_g):
  opt_g.zero_grad()

   # generate fake images

  latent = torch.rand (batch_size,latent_size,1,1, device= device)
  fake_images = generator (latent)

  # try to fool discriminator

  predictions = discriminator (fake_images)
  targets = torch.ones (batch_size,1,device=device)
  loss = F.binary_cross_entropy (predictions,targets)

  #update generator weights
  loss.backward()
  opt_g.step()
  return loss.item()