{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf496c1c",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "## exercise 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e462509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function output f(6,6): 0.0\n",
      "Gradients at (6,6): [-6.0, 6.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t00002k9\\AppData\\Local\\Temp\\ipykernel_31924\\834250013.py:26: DeprecationWarning: <class '__main__.MyFunction'> should not be instantiated. Methods on autograd functions are all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.\n",
      "  func = MyFunction()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import Tensor\n",
    "\n",
    "# ---------- Part A: Custom autograd function for f(x1,x2) ----------\n",
    "# f(x) = (x1 - 6)^2 + x2^2 - x1*x2\n",
    "class MyFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, x: Tensor) -> Tensor:\n",
    "        # Save x for backward\n",
    "        ctx.save_for_backward(x)\n",
    "        x1, x2 = x[0], x[1]\n",
    "        func_value = (x1 - 6)**2 + x2**2 - x1 * x2  # scalar tensor\n",
    "        return func_value\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_output: Tensor):\n",
    "        (x,) = ctx.saved_tensors\n",
    "        x1, x2 = x[0], x[1]\n",
    "        # ∇f = [ 2(x1-6) - x2,  2x2 - x1 ]\n",
    "        grad_x = torch.tensor([2*(x1 - 6) - x2, 2*x2 - x1], dtype=x.dtype, device=x.device)\n",
    "        # Chain rule: grad_output is scalar multiplier from subsequent ops\n",
    "        return grad_output * grad_x\n",
    "\n",
    "func = MyFunction()\n",
    "\n",
    "# Check: forward and backward at x = (6,6)\n",
    "x = torch.tensor([6., 6.], requires_grad=True)\n",
    "y = func.apply(x)\n",
    "print('Function output f(6,6):', y.item())\n",
    "y.backward()\n",
    "print('Gradients at (6,6):', x.grad.tolist())    # Expect [-6, 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e60ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  1: x = [6.0, 6.0], f(x) = 0.000000, ||grad|| = 8.485e+00\n",
      "step  2: x = [6.599999904632568, 5.400000095367432], f(x) = -6.119997, ||grad|| = 5.940e+00\n",
      "step  3: x = [7.019999980926514, 4.980000019073486], f(x) = -9.118799, ||grad|| = 4.158e+00\n",
      "step  4: x = [7.314000129699707, 4.685999870300293], f(x) = -10.588211, ||grad|| = 2.910e+00\n",
      "step  5: x = [7.519800186157227, 4.480199813842773], f(x) = -11.308224, ||grad|| = 2.037e+00\n",
      "step  6: x = [7.663860321044922, 4.336139678955078], f(x) = -11.661030, ||grad|| = 1.426e+00\n",
      "step  7: x = [7.764702320098877, 4.235297679901123], f(x) = -11.833906, ||grad|| = 9.983e-01\n",
      "step  8: x = [7.835291862487793, 4.164708137512207], f(x) = -11.918613, ||grad|| = 6.988e-01\n",
      "step  9: x = [7.884704113006592, 4.115295886993408], f(x) = -11.960123, ||grad|| = 4.892e-01\n",
      "step 10: x = [7.91929292678833, 4.08070707321167], f(x) = -11.980459, ||grad|| = 3.424e-01\n",
      "Final x: [7.94350528717041, 4.05649471282959]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t00002k9\\AppData\\Local\\Temp\\ipykernel_31924\\365956192.py:51: DeprecationWarning: <class '__main__.MyFunction'> should not be instantiated. Methods on autograd functions are all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.\n",
      "  gd_optimizer = GradientDescentOptimizer(func=MyFunction(), max_steps=10, alpha=0.1)  # alpha < 2/3 ensures convergence\n"
     ]
    }
   ],
   "source": [
    "class GradientDescentOptimizer:\n",
    "    def __init__(self, func: Function, max_steps: int, alpha: float):\n",
    "        \"\"\"\n",
    "        Init an Optimizer for performing GD.\n",
    "        :param func: Function to apply.\n",
    "        :param max_steps: Maximum number of GD steps.\n",
    "        :param alpha: Learning Rate.\n",
    "        \"\"\"\n",
    "        self.func = func\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x: Tensor, tol: float = 1e-6, verbose: bool = True) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply GD on a tensor.\n",
    "        :param x: Input tensor (will not be modified).\n",
    "        :param tol: Early-stopping threshold on gradient norm.\n",
    "        :param verbose: If True, print progress.\n",
    "        \"\"\"\n",
    "        # Work on a detached copy\n",
    "        x_cp = x.detach().clone()\n",
    "        x_cp.requires_grad_(True)\n",
    "\n",
    "        for step in range(1, self.max_steps + 1):\n",
    "            # Forward\n",
    "            y = self.func.apply(x_cp)\n",
    "\n",
    "            # Backward: compute ∇f(x_cp)\n",
    "            if x_cp.grad is not None:\n",
    "                x_cp.grad.zero_()\n",
    "            y.backward()\n",
    "\n",
    "            # Early stopping: small gradient norm\n",
    "            grad_norm = x_cp.grad.norm().item()\n",
    "            if verbose:\n",
    "                print(f\"step {step:2d}: x = {x_cp.detach().tolist()}, f(x) = {y.item():.6f}, ||grad|| = {grad_norm:.3e}\")\n",
    "\n",
    "            if grad_norm < tol:\n",
    "                if verbose:\n",
    "                    print(f\"Early stop: gradient norm {grad_norm:.3e} < tol {tol:.1e}\")\n",
    "                break\n",
    "\n",
    "            # Gradient step: x <- x - alpha * grad\n",
    "            with torch.no_grad():\n",
    "                x_cp.data -= self.alpha * x_cp.grad.data\n",
    "\n",
    "        return x_cp\n",
    "\n",
    "# Run the optimizer from x = (6,6)\n",
    "x0 = torch.tensor([6., 6.], requires_grad=True)\n",
    "gd_optimizer = GradientDescentOptimizer(func=MyFunction(), max_steps=10, alpha=0.1)  # alpha < 2/3 ensures convergence\n",
    "x_new = gd_optimizer(x0, tol=1e-10, verbose=True)\n",
    "print(\"Final x:\", x_new.detach().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
